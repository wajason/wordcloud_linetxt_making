# -*- coding: utf-8 -*-
"""文字雲製作及簡易分析.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10x-10d1TBYTXGnb2S64Y5e1HC_RYiGHc
"""

pip install pandas wordcloud matplotlib

pip install nltk

import nltk

nltk.download('vader_lexicon')

"""## 清理及生成新檔案

"""

import re

def clean_text(file_path):
    with open('chat_records.txt', 'r', encoding='utf-8') as file:
        lines = file.readlines()

    cleaned_lines = []
    for line in lines:
        line = line.strip()
        # 刪除時間訊息
        line = re.sub(r'^\d{2}:\d{2}\t', '', line)
        # 刪除使用者名稱
        line = re.sub(r'^\w+\t', '', line)
        # 刪除已收回訊息
        line = re.sub(r'\t您已收回訊息$', '', line)
        line = re.sub(r'\t\w+已收回訊息$', '', line)
        # 刪除連結網址
        line = re.sub(r'https?://\S+', '', line)

        cleaned_lines.append(line)

    cleaned_text = '\n'.join(cleaned_lines)
    new_file_path = file_path.replace('.txt', '_cleaned.txt')

    with open(new_file_path, 'w', encoding='utf-8') as new_file:
        new_file.write(cleaned_text)

    return new_file_path

file_path = 'cleaned_chat.txt'  # 將 'your_file_path.txt' 替換為你的文本檔案路徑
cleaned_file_path = clean_text(file_path)
print(f'已生成新檔案：{cleaned_file_path}')

"""## 匯入中文字型"""

!pip install wget
import wget
wget.download('https://raw.githubusercontent.com/GrandmaCan/ML/main/Resgression/ChineseFont.ttf')

import matplotlib as mlp
from matplotlib.font_manager import fontManager
fontManager.addfont("ChineseFont.ttf")
mlp.rc('font', family="ChineseFont")

"""## 詞語出現次數整理檔案"""

import re
from collections import Counter

# 讀取清理後的文本文件
with open('cleaned_chat_cleaned.txt', 'r', encoding='utf-8') as file:
    cleaned_text = file.read()

# 獲取詞語出現次數
words = re.findall(r'\b\w+\b', cleaned_text)
word_counts = Counter(words)

# 按照詞語出現次數由大到小排序
sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)

# 寫入詞語出現次數整體檔案
with open('word_counts.txt', 'w', encoding='utf-8') as file:
    for word, count in sorted_word_counts:
        file.write(f'{word}: {count}\n')

print('詞語出現次數整理檔案已生成。')

"""## 文字雲產生"""

import re
import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from matplotlib.font_manager import FontProperties
from collections import Counter


# 讀取清理後的文本文件
with open('cleaned_chat_cleaned.txt', 'r', encoding='utf-8') as file:
    cleaned_text = file.read()

# 獲取詞語出現次數
words = re.findall(r'\b\w+\b', cleaned_text)
word_counts = Counter(words)

# 設置中文字體路徑
font_path = 'ChineseFont.ttf'  # 將 'ChineseFont.ttf' 替換為你的中文字體檔案路徑

# 計算最大出現次數
max_count = max(word_counts.values())

# 建立文字雲物件，設置中文字體，並根據詞語出現次數生成文字雲
wordcloud = WordCloud(width=800, height=400, background_color='white', font_path=font_path)

# 建立自訂字體大小函式
def relative_font_size(count):
    return int(100 + 300 * count / max_count)  # 調整數值範圍以控制字體大小

# 調整文字雲的詞語大小
wordcloud.generate_from_frequencies({word: relative_font_size(count) for word, count in word_counts.items()})

# 繪製文字雲
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')

# 顯示文字雲
plt.show()

"""## 文字雲簡易分析

"""

import re
from collections import Counter
from nltk import bigrams
from nltk.sentiment import SentimentIntensityAnalyzer
import pandas as pd
import matplotlib.pyplot as plt


# 讀取清理後的文本文件
with open('cleaned_chat_cleaned.txt', 'r', encoding='utf-8') as file:
    cleaned_text = file.read()

# 獲取詞語出現次數
words = re.findall(r'\b\w+\b', cleaned_text)
word_counts = Counter(words)

# 提取詞語關聯性
word_pairs = list(bigrams(words))

# 分析詞彙關聯性
most_common_pairs = Counter(word_pairs).most_common(30)

# 輸出詞彙關聯性分析結果
print("詞彙關聯性分析結果:")
for pair, count in most_common_pairs:
    print(f"{pair}: {count}")

# 進行情緒分析
sentiment_analyzer = SentimentIntensityAnalyzer()

# 分析每個詞的情緒分數
word_sentiments = {}
for word in word_counts.keys():
    sentiment = sentiment_analyzer.polarity_scores(word)
    word_sentiments[word] = sentiment['compound']

# 輸出詞彙情緒分析結果
print("詞彙情緒分析結果:")
for word, sentiment in word_sentiments.items():
    print(f"{word}: {sentiment}")

# 將詞彙關聯性分析結果轉換成DataFrame
df_pairs = pd.DataFrame(most_common_pairs, columns=['Word Pair', 'Count'])

# 輸出詞彙關聯性分析結果表格
print("詞彙關聯性分析結果:")
print(df_pairs)

# 將詞彙情緒分析結果轉換成DataFrame
df_sentiments = pd.DataFrame.from_dict(word_sentiments, orient='index', columns=['Sentiment'])

# 添加詞彙列
df_sentiments['Word'] = df_sentiments.index

# 去除情緒為0.0的詞彙
df_sentiments = df_sentiments[df_sentiments['Sentiment'] != 0.0]

# 輸出詞彙情緒分析結果表格
print("詞彙情緒分析結果:")
print(df_sentiments)

# 繪製詞彙關聯性長條圖
plt.figure(figsize=(10, 5))
plt.bar(df_pairs['Word Pair'].apply(lambda x: ' '.join(x)), df_pairs['Count'])
plt.xlabel('Word Pair')
plt.ylabel('Count')
plt.title('Word Pair Association')
plt.xticks(rotation=90)
plt.axhline(0, color='black', linewidth=0.5)  # 添加分隔線
plt.show()

# 繪製詞彙情緒分數折線圖
plt.figure(figsize=(10, 5))
plt.plot(df_sentiments['Word'], df_sentiments['Sentiment'])
plt.xlabel('Word')
plt.ylabel('Sentiment Score')
plt.title('Word Sentiment Analysis')
plt.xticks(rotation=90)
plt.axhline(0, color='black', linewidth=0.5)  # 添加分隔線
plt.show()